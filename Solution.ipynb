{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are scraping can be found here:\n",
    "\n",
    "https://mafudge.github.io/web-scraping/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading an HTML table\n",
    "\n",
    "Is trivial, thanks to Pandas. The read_html method returns a list of all tables on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table read example\n",
    "tables = pd.read_html(\"https://mafudge.github.io/web-scraping/emptable.html\")\n",
    "table = tables[0] # read_html returns a list of all tables on the page.\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read JSON Data \n",
    "\n",
    "Is also trivial... Thanks to Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON example\n",
    "data = pd.read_json(\"https://mafudge.github.io/web-scraping/empjson.json\")\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or this if you don't want Pandas... \n",
    "response = requests.get(\"https://mafudge.github.io/web-scraping/empjson.json\")\n",
    "data = json.loads(response.text) \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Data \n",
    "\n",
    "Is a bit more involved because we must use Python logic to get the structure off the page\n",
    "\n",
    "The document we are scraping is here: https://mafudge.github.io/web-scraping/empweb.html\n",
    "\n",
    "You can't scrape without the knowing the HTML structure. When the HTML changes, we must re-write our code. This is why an API is always preferable to scraping. Web scraping is a method of last resort!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we request the contents of the page and send them to Beautiful Soup\n",
    "response = requests.get(\"https://mafudge.github.io/web-scraping/empweb.html\")\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "print(str(soup)[:200]) # there's a lot, so only print the first 200 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get some departments, which are in <h3> tags:\n",
    "for h3_tag in soup.select(\"h3\"):\n",
    "    print(h3_tag.text)\n",
    "    \n",
    "# select() searches the html for that tag, returning a list\n",
    "print(soup.select(\"h3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the employees are in li tags:\n",
    "for li_tag in soup.select(\"li\"):\n",
    "    print(li_tag.text)\n",
    "    \n",
    "print(soup.select(\"li\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do you associate the employee with their department?\n",
    "# the tags are nested, and so must code be!\n",
    "h3_tags = soup.select(\"h3\") #get the departments as a list \n",
    "tag_index =0\n",
    "for ul_tag in soup.select(\"ul\"):\n",
    "    for li_tag in ul_tag.select(\"li\"):\n",
    "        print(\"Name: {0} Department: {1}\".format(\n",
    "            li_tag.text, \n",
    "            h3_tags[tag_index].text)\n",
    "             )\n",
    "    tag_index+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how do we extract the email? Well that's part of the a href\n",
    "print(\"Entire list item =======> \", li_tag)\n",
    "print(\"Anchor (a) Tag inside ==> \", li_tag.select_one(\"a\")) # do not return a list in this case\n",
    "print(\"Href attribute in (a) ==> \", li_tag.select_one(\"a\")[\"href\"]) #dictionary key\n",
    "print(\"Strip out :mailto ======> \", li_tag.select_one(\"a\")[\"href\"].replace(\"mailto:\",\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so much logic here, belongs in its own function for readability\n",
    "def get_email_from_li(tag):\n",
    "    return tag.select_one(\"a\")[\"href\"].replace(\"mailto:\",\"\")\n",
    "\n",
    "# testing\n",
    "print(get_email_from_li(li_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we write the entire scrape as:\n",
    "h3_tags = soup.select(\"h3\") \n",
    "tag_index =0\n",
    "for ul_tag in soup.select(\"ul\"):\n",
    "    for li_tag in ul_tag.select(\"li\"):\n",
    "        print(\"Name: {0} Department: {1} Email: {2}\".format(\n",
    "            li_tag.text, \n",
    "            h3_tags[tag_index].text,\n",
    "            get_email_from_li(li_tag))\n",
    "             )\n",
    "    tag_index+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of dictionary is more workable than printing, of course\n",
    "employees = []\n",
    "h3_tags = soup.select(\"h3\") \n",
    "tag_index =0\n",
    "for ul_tag in soup.select(\"ul\"):\n",
    "    for li_tag in ul_tag.select(\"li\"):\n",
    "        # build a dictionary\n",
    "        employee = { \n",
    "            \"Name\" : li_tag.text, \n",
    "            \"Department\" : h3_tags[tag_index].text,\n",
    "            \"Email\" : get_email_from_li(li_tag)\n",
    "        }\n",
    "        # add to the list\n",
    "        employees.append(employee)\n",
    "    tag_index+=1 \n",
    "    \n",
    "data = pd.DataFrame(employees)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about writing this as a function to return a python list of dictionary\n",
    "def scrape_fudgemart_employees():\n",
    "    response = requests.get(\"https://mafudge.github.io/web-scraping/empweb.html\")\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    employees = []\n",
    "    h3_tags = soup.select(\"h3\") \n",
    "    tag_index =0\n",
    "    for ul_tag in soup.select(\"ul\"):\n",
    "        for li_tag in ul_tag.select(\"li\"):\n",
    "            employee = { \n",
    "                \"Name\" : li_tag.text, \n",
    "                \"Department\" : h3_tags[tag_index].text,\n",
    "                \"Email\" : get_email_from_li(li_tag)\n",
    "            }\n",
    "            employees.append(employee)\n",
    "        tag_index+=1 \n",
    "    return employees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now it's trivial :-)\n",
    "employees = scrape_fudgemart_employees()\n",
    "data = pd.DataFrame(employees)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
